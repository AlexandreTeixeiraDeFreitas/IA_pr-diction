{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRA_EMAIL = \"xxxxxxxxxxxxx.xxxxxxxxxxxxx@xxxxxxxxxx.com\"\n",
    "JIRA_TOKEN = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "JIRA_URL = \"https://xxxxxxxxxxxxxx.atlassian.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "00bca972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_filtered_jira_issues(df_tickets):\n",
    "    \"\"\"\n",
    "    Récupère les tickets Jira correspondant aux composants, types et champs personnalisés d'un DataFrame :\n",
    "    - dont \"fields.resolution.name\" == \"Done\"\n",
    "    - dont \"fields.components\" contient au moins un composant présent dans df_tickets\n",
    "    - dont \"fields.customfield_10116.value\" contient une valeur présente dans df_tickets (filtré après)\n",
    "    - dont \"fields.issuetype.name\" correspond à un type présent dans df_tickets\n",
    "    - créés dans l'année précédente par rapport à leur propre date de création\n",
    "\n",
    "    Args:\n",
    "        df_tickets (pd.DataFrame): Un DataFrame avec des colonnes Jira standards\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Tickets filtrés\n",
    "    \"\"\"\n",
    "    if \"key\" not in df_tickets.columns:\n",
    "        raise ValueError(\"Le DataFrame doit contenir une colonne 'key' avec les identifiants Jira\")\n",
    "\n",
    "    auth = (JIRA_EMAIL, JIRA_TOKEN)\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    results = []\n",
    "\n",
    "    # Extraction des composants, types d'issue et valeurs custom à réutiliser\n",
    "    component_set = set()\n",
    "    issuetype_set = set()\n",
    "    custom_value_set = set()\n",
    "\n",
    "    if \"fields.components\" in df_tickets.columns:\n",
    "        df_tickets[\"fields.components\"].dropna().apply(\n",
    "            lambda lst: [component_set.add(comp[\"name\"]) for comp in lst if isinstance(comp, dict)]\n",
    "        )\n",
    "    if \"fields.issuetype.name\" in df_tickets.columns:\n",
    "        issuetype_set = set(df_tickets[\"fields.issuetype.name\"].dropna().tolist())\n",
    "    if \"fields.customfield_10116.value\" in df_tickets.columns:\n",
    "        custom_value_set = set(df_tickets[\"fields.customfield_10116.value\"].dropna().tolist())\n",
    "\n",
    "    url = f\"{JIRA_URL}/rest/api/latest/search\"\n",
    "\n",
    "    for issuetype in issuetype_set:\n",
    "        for component in component_set:\n",
    "            jql = f\"created >= -365d AND component = \\\"{component}\\\" AND issuetype = \\\"{issuetype}\\\" AND resolution = Done\"\n",
    "            start_at = 0\n",
    "\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"jql\": jql,\n",
    "                    \"startAt\": start_at,\n",
    "                    \"maxResults\": 100\n",
    "                }\n",
    "\n",
    "                response = requests.get(url, auth=auth, headers=headers, params=params)\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠️ Erreur API Jira : {response.status_code} {response.text}\")\n",
    "                    break\n",
    "\n",
    "                data = response.json()\n",
    "                issues = data.get(\"issues\", [])\n",
    "                total = data.get(\"total\", 0)\n",
    "\n",
    "                for issue in issues:\n",
    "                    fields = issue.get(\"fields\", {})\n",
    "                    resolution = fields.get(\"resolution\", {}).get(\"name\", \"\")\n",
    "                    components = fields.get(\"components\", [])\n",
    "                    custom_value = fields.get(\"customfield_10116\", {}).get(\"value\", None)\n",
    "                    issuetype_value = fields.get(\"issuetype\", {}).get(\"name\", None)\n",
    "                    created = fields.get(\"created\", \"\")\n",
    "\n",
    "                    if not created:\n",
    "                        continue\n",
    "\n",
    "                    created_date = datetime.strptime(created[:10], \"%Y-%m-%d\")\n",
    "                    date_min = created_date - timedelta(days=365)\n",
    "\n",
    "                    component_names = [comp.get(\"name\") for comp in components if isinstance(comp, dict)]\n",
    "                    has_common_component = bool(component_set.intersection(component_names))\n",
    "                    has_common_custom_value = custom_value in custom_value_set\n",
    "                    has_common_issuetype = issuetype_value in issuetype_set\n",
    "\n",
    "                    if (resolution == \"Done\" and\n",
    "                        has_common_component and\n",
    "                        has_common_issuetype and\n",
    "                        date_min <= created_date <= created_date):\n",
    "\n",
    "                        flat = pd.json_normalize(issue)\n",
    "                        if has_common_custom_value:\n",
    "                            results.append(flat)\n",
    "\n",
    "                start_at += len(issues)\n",
    "                if start_at >= total:\n",
    "                    break\n",
    "\n",
    "    if results:\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e3a50cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              expand       id  \\\n",
      "0  renderedFields,names,schema,operations,editmet...  7974132   \n",
      "\n",
      "                                                self        key  \\\n",
      "0  https://arvato-scs.atlassian.net/rest/api/late...  CMH-15279   \n",
      "\n",
      "  fields.parent.id fields.parent.key  \\\n",
      "0          2626824         CMH-10426   \n",
      "\n",
      "                                  fields.parent.self  \\\n",
      "0  https://arvato-scs.atlassian.net/rest/api/2/is...   \n",
      "\n",
      "  fields.parent.fields.summary  \\\n",
      "0             431 BMS - France   \n",
      "\n",
      "                    fields.parent.fields.status.self  \\\n",
      "0  https://arvato-scs.atlassian.net/rest/api/2/st...   \n",
      "\n",
      "  fields.parent.fields.status.description  ... fields.customfield_10080  \\\n",
      "0                                          ...                     None   \n",
      "\n",
      "  fields.customfield_10081 fields.customfield_10082 fields.customfield_12382  \\\n",
      "0                     None                     None                     None   \n",
      "\n",
      "   fields.customfield_10087 fields.customfield_10088 fields.customfield_14205  \\\n",
      "0                        []                     None                     None   \n",
      "\n",
      "  fields.customfield_14206 fields.customfield_14204 fields.customfield_12269  \n",
      "0                     None                     None                     None  \n",
      "\n",
      "[1 rows x 590 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "auth = (JIRA_EMAIL, JIRA_TOKEN)\n",
    "headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# Ticket à récupérer\n",
    "ticket_id = \"CMH-15279\"\n",
    "url = f\"{JIRA_URL}/rest/api/latest/issue/{ticket_id}\"\n",
    "\n",
    "# Appel API\n",
    "response = requests.get(url, auth=auth, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Transformation en DataFrame\n",
    "issue_data = response.json()\n",
    "df_ticket = pd.json_normalize(issue_data)\n",
    "print(df_ticket.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "426c66dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atdf2\\AppData\\Local\\Temp\\ipykernel_21772\\3261151567.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(results, ignore_index=True)\n",
      "C:\\Users\\atdf2\\AppData\\Local\\Temp\\ipykernel_21772\\3602510204.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all_ticket = pd.concat([df_ticket, df_all_ticket], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# # Ajoute la colonne 'Matricule' si elle n'existe pas dans df_ticket\n",
    "# if \"Matricule\" not in df_ticket.columns:\n",
    "#     matricule = \"900265\"  # ou récupéré dynamiquement d'un formulaire ou input utilisateur\n",
    "#     df_ticket[\"Matricule\"] = matricule\n",
    "\n",
    "# Appel API pour tickets similaires\n",
    "df_all_ticket = get_filtered_jira_issues(df_ticket)\n",
    "\n",
    "# Fusion des tickets initiaux et similaires\n",
    "df_all_ticket = pd.concat([df_ticket, df_all_ticket], ignore_index=True)\n",
    "\n",
    "# Suppression des doublons par clé unique Jira\n",
    "df_all_ticket = df_all_ticket.drop_duplicates(subset=\"key\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f43f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def lire_zh12_depuis_sqlite(sqlite_path, table_name, jira_keys):\n",
    "    \"\"\"\n",
    "    Charge uniquement les lignes de la base SQLite dont la colonne Jira est dans jira_keys.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(sqlite_path)\n",
    "\n",
    "    # Convertir les clés Jira en tuple SQL-safe pour l'IN clause\n",
    "    placeholders = ','.join(['?'] * len(jira_keys))\n",
    "    query = f\"SELECT * FROM {table_name} WHERE Jira IN ({placeholders})\"\n",
    "\n",
    "    zh12 = pd.read_sql_query(query, conn, params=tuple(jira_keys))\n",
    "    conn.close()\n",
    "    return zh12\n",
    "\n",
    "def tester_requete_sql(sqlite_path: str, table: str, tickets):\n",
    "    \"\"\"\n",
    "    Exécute une requête SELECT * sur une table SQLite filtrée par une ou plusieurs clés Jira.\n",
    "    \n",
    "    Args:\n",
    "        sqlite_path (str): Chemin vers la base SQLite\n",
    "        table (str): Nom de la table (ex: \"zh12\")\n",
    "        tickets (str | list[str]): Ticket Jira ou liste de tickets\n",
    "        max_lignes (int): Nombre max de lignes à afficher\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(sqlite_path)\n",
    "        print(f\"📡 Connexion ouverte vers {sqlite_path}\")\n",
    "\n",
    "        # Préparation des tickets\n",
    "        if isinstance(tickets, str):\n",
    "            tickets = [tickets]\n",
    "        tickets = [str(t).strip() for t in tickets if t]\n",
    "\n",
    "        # Clause WHERE Jira IN (?, ?, ...)\n",
    "        placeholders = ','.join(['?'] * len(tickets))\n",
    "        requete_sql = f\"SELECT * FROM {table} WHERE Jira IN ({placeholders})\"\n",
    "\n",
    "        print(\"🔎 Requête préparée :\")\n",
    "        print(f\"{requete_sql}\")\n",
    "        print(\"🔑 Paramètres :\", tickets)\n",
    "\n",
    "        df = pd.read_sql_query(requete_sql, conn, params=tuple(tickets))\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"✅ {len(df)} ligne(s) récupérée(s).\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def construire_type_etendu(row, colonnes):\n",
    "    valeurs = []\n",
    "    for col in colonnes:\n",
    "        val = str(row.get(col, \"\")).strip()\n",
    "        if val:\n",
    "            valeurs.append(val)\n",
    "    return \"__\".join(valeurs) if valeurs else None\n",
    "\n",
    "def calculer_historique_1an(df, df_reference, type_col=\"Type_Étendu\", date_col=\"fields.created\", projet_col=\"fields.project.key\", duree_col=\"Duree\"):\n",
    "    history = []\n",
    "    for idx, row in df.iterrows():\n",
    "        typ = row[type_col]\n",
    "        date = row[date_col]\n",
    "        projet = row[projet_col]\n",
    "\n",
    "        if pd.isna(typ) or pd.isna(date) or pd.isna(projet):\n",
    "            history.append(0)\n",
    "            continue\n",
    "\n",
    "        date_min = date - timedelta(days=365)\n",
    "        past = df_reference[\n",
    "            (df_reference[projet_col] == projet) &\n",
    "            (df_reference[date_col] < date) &\n",
    "            (df_reference[date_col] >= date_min) &\n",
    "            (df_reference[type_col] == typ)\n",
    "        ]\n",
    "        print(f\"🔍 Historique pour {typ} dans {projet} entre {date_min} et {date}: {len(past)} entrées trouvées\")\n",
    "        mean_val = past[duree_col].mean() if not past.empty else 0\n",
    "        history.append(mean_val)\n",
    "\n",
    "    return history\n",
    "\n",
    "# --------- MAIN PIPELINE ---------\n",
    "\n",
    "def traiter_historique_1an(df_ticket: pd.DataFrame, df_merged: pd.DataFrame, zh12_path: str, date_commencement: str, matricule: str):\n",
    "    \"\"\"\n",
    "    Calcule l'historique glissant 1 an pour les tickets Jira (df_ticket) enrichis via df_merged.\n",
    "    Ajoute les tickets manquants dans ZH12 avec durée = 0.\n",
    "    \"\"\"\n",
    "    # Copie des tickets enrichis\n",
    "    jira_df = df_merged.copy()\n",
    "\n",
    "    # Charger ZH12 uniquement pour les clés présentes dans Jira\n",
    "    keep_keys = list(set(jira_df[\"key\"]))\n",
    "    zh12 = lire_zh12_depuis_sqlite(zh12_path, \"zh12\", keep_keys)\n",
    "    print(f\"📥 Chargement de {len(zh12)} lignes de ZH12 pour {len(keep_keys)} tickets Jira\")\n",
    "\n",
    "    # Agrégation ZH12 si non vide\n",
    "    if not zh12.empty:\n",
    "        zh12[\"Durée tâche (heures)\"] = pd.to_numeric(zh12[\"Durée tâche (heures)\"], errors=\"coerce\")\n",
    "        zh12[\"Date\"] = pd.to_datetime(zh12[\"Date\"], errors=\"coerce\")\n",
    "        aggr = zh12.groupby([\"Matricule\", \"Jira\"], as_index=False).agg({\n",
    "            \"Durée tâche (heures)\": \"sum\",\n",
    "            \"Code Service\": \"first\",\n",
    "            \"Date\": \"min\"\n",
    "        }).rename(columns={\"Date\": \"Date commence\"})\n",
    "    else:\n",
    "        aggr = pd.DataFrame(columns=[\"Matricule\", \"Jira\", \"Durée tâche (heures)\", \"Code Service\", \"Date commence\"])\n",
    "\n",
    "    # Ajouter les tickets manquants (non déjà dans aggr)\n",
    "    missing_keys = [k for k in df_ticket[\"key\"] if k not in aggr[\"Jira\"].values]\n",
    "\n",
    "    if missing_keys:\n",
    "        rows_to_add = pd.DataFrame({\n",
    "            \"Matricule\": [matricule] * len(missing_keys),\n",
    "            \"Jira\": missing_keys,\n",
    "            \"Durée tâche (heures)\": [0] * len(missing_keys),\n",
    "            \"Code Service\": [None] * len(missing_keys),\n",
    "            \"Date commence\": [date_commencement] * len(missing_keys)\n",
    "        })\n",
    "        aggr = pd.concat([aggr, rows_to_add], ignore_index=True)\n",
    "    print(aggr.head())\n",
    "\n",
    "    # Merge avec les données Jira\n",
    "    merged_df = aggr.merge(jira_df, left_on=\"Jira\", right_on=\"key\", how=\"left\")\n",
    "    print(f\"🔗 Fusion des données Jira et ZH12 : {len(merged_df)} lignes après fusion\")\n",
    "\n",
    "    # Nettoyage des composants\n",
    "    merged_df[\"fields.components\"] = merged_df[\"fields.components\"].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "    )\n",
    "    merged_df[\"components\"] = merged_df[\"fields.components\"].apply(\n",
    "        lambda comps: \" / \".join(\n",
    "            [c[\"name\"] for c in comps if isinstance(c, dict) and \"name\" in c]\n",
    "        ) if isinstance(comps, list) else None\n",
    "    )\n",
    "\n",
    "    # Construction du type étendu\n",
    "    colonnes_type = [\"fields.issuetype.name\", \"components\", \"fields.customfield_10116.value\"]\n",
    "    merged_df[\"Type_Étendu\"] = merged_df.apply(lambda r: construire_type_etendu(r, colonnes_type), axis=1)\n",
    "    merged_df = merged_df.dropna(subset=[\"Type_Étendu\"])\n",
    "    merged_df[\"Type_Étendu\"] = merged_df[\"Type_Étendu\"].astype(str)\n",
    "    merged_df = merged_df[merged_df[\"Type_Étendu\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "    # Nettoyage des dates\n",
    "    merged_df[\"fields.created\"] = pd.to_datetime(merged_df[\"fields.created\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "    print(merged_df[[\"Type_Étendu\", \"fields.created\"]])\n",
    "\n",
    "    # Appliquer Date commence pour les lignes de df_ticket\n",
    "    date_val = pd.to_datetime(date_commencement)\n",
    "    keys_ticket = set(df_ticket[\"key\"])\n",
    "    mask = merged_df[\"key\"].isin(keys_ticket)\n",
    "    merged_df.loc[mask, \"Date commence\"] = date_val\n",
    "    merged_df[\"Date commence\"] = pd.to_datetime(merged_df[\"Date commence\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "    # Colonnes temporelles\n",
    "    merged_df[\"annee_creation\"] = merged_df[\"fields.created\"].dt.year\n",
    "    merged_df[\"mois_creation\"] = merged_df[\"fields.created\"].dt.month\n",
    "    merged_df[\"jour_semaine\"] = merged_df[\"fields.created\"].dt.weekday\n",
    "    merged_df[\"delai_creation_action_h\"] = (\n",
    "        (merged_df[\"Date commence\"] - merged_df[\"fields.created\"]).dt.total_seconds() / 3600\n",
    "    )\n",
    "\n",
    "    merged_df.rename(columns={\"Durée tâche (heures)\": \"Duree\"}, inplace=True)\n",
    "\n",
    "    # Calcul Historique uniquement sur les tickets initiaux\n",
    "    merged_df[\"Historique_1an\"] = 0\n",
    "    # subset_to_compute = merged_df[mask].copy()\n",
    "    merged_df[mask][\"Historique_1an\"] = calculer_historique_1an(merged_df[mask], merged_df)\n",
    "\n",
    "    return merged_df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "35f6e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Chargement de 1 lignes de ZH12 pour 10 tickets Jira\n",
      "  Matricule       Jira  Durée tâche (heures)  Code Service Date commence\n",
      "0   7000755  CMH-15279                   1.0  SCM-IT-FR-HC    2025-06-04\n",
      "🔗 Fusion des données Jira et ZH12 : 1 lignes après fusion\n",
      "                         Type_Étendu          fields.created\n",
      "0  Change__JP6 - 431 BMS__Healthcare 2025-06-03 12:22:56.417\n",
      "🔍 Historique pour Change__JP6 - 431 BMS__Healthcare dans CMH entre 2024-06-03 12:22:56.417000 et 2025-06-03 12:22:56.417000: 0 entrées trouvées\n"
     ]
    }
   ],
   "source": [
    "result = traiter_historique_1an(\n",
    "    df_ticket=df_ticket,\n",
    "    df_merged=df_all_ticket,\n",
    "    zh12_path=\"zh12_v2.sqlite\",\n",
    "    date_commencement=\"2025-07-13\",\n",
    "    matricule=\"900265\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "57c0ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matricule</th>\n",
       "      <th>Jira</th>\n",
       "      <th>Duree</th>\n",
       "      <th>Code Service</th>\n",
       "      <th>Date commence</th>\n",
       "      <th>expand</th>\n",
       "      <th>id</th>\n",
       "      <th>self</th>\n",
       "      <th>key</th>\n",
       "      <th>fields.parent.id</th>\n",
       "      <th>...</th>\n",
       "      <th>fields.customfield_10126.value</th>\n",
       "      <th>fields.customfield_10126.id</th>\n",
       "      <th>fields.customfield_10104</th>\n",
       "      <th>components</th>\n",
       "      <th>Type_Étendu</th>\n",
       "      <th>annee_creation</th>\n",
       "      <th>mois_creation</th>\n",
       "      <th>jour_semaine</th>\n",
       "      <th>delai_creation_action_h</th>\n",
       "      <th>Historique_1an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000755</td>\n",
       "      <td>CMH-15279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SCM-IT-FR-HC</td>\n",
       "      <td>2025-07-13</td>\n",
       "      <td>renderedFields,names,schema,operations,editmet...</td>\n",
       "      <td>7974132</td>\n",
       "      <td>https://arvato-scs.atlassian.net/rest/api/late...</td>\n",
       "      <td>CMH-15279</td>\n",
       "      <td>2626824</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JP6 - 431 BMS</td>\n",
       "      <td>Change__JP6 - 431 BMS__Healthcare</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>947.617662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Matricule       Jira  Duree  Code Service Date commence  \\\n",
       "0   7000755  CMH-15279    1.0  SCM-IT-FR-HC    2025-07-13   \n",
       "\n",
       "                                              expand       id  \\\n",
       "0  renderedFields,names,schema,operations,editmet...  7974132   \n",
       "\n",
       "                                                self        key  \\\n",
       "0  https://arvato-scs.atlassian.net/rest/api/late...  CMH-15279   \n",
       "\n",
       "  fields.parent.id  ... fields.customfield_10126.value  \\\n",
       "0          2626824  ...                            NaN   \n",
       "\n",
       "  fields.customfield_10126.id fields.customfield_10104     components  \\\n",
       "0                         NaN                      NaN  JP6 - 431 BMS   \n",
       "\n",
       "                         Type_Étendu annee_creation mois_creation  \\\n",
       "0  Change__JP6 - 431 BMS__Healthcare           2025             6   \n",
       "\n",
       "  jour_semaine delai_creation_action_h  Historique_1an  \n",
       "0            1              947.617662               0  \n",
       "\n",
       "[1 rows x 701 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "dfb4677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "\n",
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "\n",
    "# def convertir_type_sql(pandas_dtype):\n",
    "#     \"\"\"\n",
    "#     Convertit un type pandas (dtype) en type SQL standard.\n",
    "#     \"\"\"\n",
    "#     if pd.api.types.is_integer_dtype(pandas_dtype):\n",
    "#         return \"INTEGER\"\n",
    "#     elif pd.api.types.is_float_dtype(pandas_dtype):\n",
    "#         return \"REAL\"\n",
    "#     else:\n",
    "#         return \"TEXT\"\n",
    "\n",
    "# def importer_excel_dans_sqlite(excel_path, sqlite_path, table_name=\"zh12\"):\n",
    "#     # Lecture du fichier Excel avec inférence des types\n",
    "#     df = pd.read_excel(excel_path)\n",
    "#     df = df.dropna(subset=[\"Matricule\", \"Date\", \"# de tâche\"])\n",
    "\n",
    "#     # Inférer les types de chaque colonne pour SQL\n",
    "#     schema_sql = []\n",
    "#     for col in df.columns:\n",
    "#         sql_type = convertir_type_sql(df[col].dtype)\n",
    "#         schema_sql.append(f\"[{col}] {sql_type}\")\n",
    "\n",
    "#     # Ajout de la clé primaire composite\n",
    "#     schema_sql.append(\"PRIMARY KEY (Matricule, Date, [# de tâche])\")\n",
    "#     create_stmt = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n  \" + \",\\n  \".join(schema_sql) + \"\\n)\"\n",
    "\n",
    "#     # Connexion SQLite\n",
    "#     conn = sqlite3.connect(sqlite_path)\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(create_stmt)\n",
    "#     conn.commit()\n",
    "\n",
    "#     # Supprimer les doublons existants\n",
    "#     for _, row in df.iterrows():\n",
    "#         cursor.execute(f\"\"\"\n",
    "#             DELETE FROM {table_name}\n",
    "#             WHERE Matricule = ? AND Date = ? AND [# de tâche] = ?\n",
    "#         \"\"\", (str(row[\"Matricule\"]), str(row[\"Date\"]), str(row[\"# de tâche\"])))\n",
    "\n",
    "#     # Réinsertion\n",
    "#     df.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "#     print(f\"✅ {len(df)} lignes insérées dans '{table_name}' (avec remplacement des doublons).\")\n",
    "\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "\n",
    "\n",
    "# importer_excel_dans_sqlite(\"export_ZH12.xlsx\", \"zh12_v2.sqlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b964de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def tester_requete_sql(sqlite_path: str, table: str, tickets, max_lignes: int = 10):\n",
    "    \"\"\"\n",
    "    Exécute une requête SELECT * sur une table SQLite filtrée par une ou plusieurs clés Jira.\n",
    "    \n",
    "    Args:\n",
    "        sqlite_path (str): Chemin vers la base SQLite\n",
    "        table (str): Nom de la table (ex: \"zh12\")\n",
    "        tickets (str | list[str]): Ticket Jira ou liste de tickets\n",
    "        max_lignes (int): Nombre max de lignes à afficher\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(sqlite_path)\n",
    "        print(f\"📡 Connexion ouverte vers {sqlite_path}\")\n",
    "\n",
    "        # Préparation des tickets\n",
    "        if isinstance(tickets, str):\n",
    "            tickets = [tickets]\n",
    "        tickets = [str(t).strip() for t in tickets if t]\n",
    "\n",
    "        # Clause WHERE Jira IN (?, ?, ...)\n",
    "        placeholders = ','.join(['?'] * len(tickets))\n",
    "        requete_sql = f\"SELECT * FROM {table} WHERE Jira IN ({placeholders}) LIMIT {max_lignes}\"\n",
    "\n",
    "        print(\"🔎 Requête préparée :\")\n",
    "        print(f\"{requete_sql}\")\n",
    "        print(\"🔑 Paramètres :\", tickets)\n",
    "\n",
    "        df = pd.read_sql_query(requete_sql, conn, params=tuple(tickets))\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"✅ {len(df)} ligne(s) récupérée(s).\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur : {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f0d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Connexion ouverte vers zh12_v2.sqlite\n",
      "🔎 Requête préparée :\n",
      "SELECT * FROM zh12 WHERE Jira IN (?,?) LIMIT 10\n",
      "🔑 Paramètres : ['CMH-15279', 'CMH-13626']\n",
      "✅ 1 ligne(s) récupérée(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matricule</th>\n",
       "      <th>Date</th>\n",
       "      <th># de tâche</th>\n",
       "      <th>Code tâche</th>\n",
       "      <th>Jira</th>\n",
       "      <th>Code Service</th>\n",
       "      <th>Heures déclarées</th>\n",
       "      <th>Durée tâche (heures)</th>\n",
       "      <th>Unité quantité base</th>\n",
       "      <th>Créé par</th>\n",
       "      <th>Saisi le</th>\n",
       "      <th>Heure de création</th>\n",
       "      <th>Confirmé</th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Durée tâche (heures).1</th>\n",
       "      <th>Unité quantité base.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000755</td>\n",
       "      <td>2025-06-04 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>PCBMS</td>\n",
       "      <td>CMH-15279</td>\n",
       "      <td>SCM-IT-FR-HC</td>\n",
       "      <td>01:00:00.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NGUY169</td>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>18:08:22.000000</td>\n",
       "      <td>X</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Matricule                 Date  # de tâche Code tâche       Jira  \\\n",
       "0   7000755  2025-06-04 00:00:00           2      PCBMS  CMH-15279   \n",
       "\n",
       "   Code Service Heures déclarées  Durée tâche (heures) Unité quantité base  \\\n",
       "0  SCM-IT-FR-HC  01:00:00.000000                   1.0                   H   \n",
       "\n",
       "  Créé par             Saisi le Heure de création Confirmé Commentaire  \\\n",
       "0  NGUY169  2025-06-30 00:00:00   18:08:22.000000        X        None   \n",
       "\n",
       "   Durée tâche (heures).1 Unité quantité base.1  \n",
       "0                       0                  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un seul ticket\n",
    "# df_test = tester_requete_sql(\"zh12_v2.sqlite\", \"zh12\", \"CMH-15279\")\n",
    "\n",
    "# # Ou plusieurs tickets\n",
    "df_test = tester_requete_sql(\"zh12_v2.sqlite\", \"zh12\", [\"CMH-15279\", \"CMH-13626\"])\n",
    "\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ia_tickets_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
